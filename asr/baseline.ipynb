{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "spotter_denoiser_challenge.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/VadimKaryakin/mlcup/blob/main/asr/baseline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdLnh3o8zxjs"
      },
      "source": [
        "# Голосовая активация на шумных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOFy9gtC6MlX"
      },
      "source": [
        "#!sudo apt-get install sox libsox-dev libsox-fmt-all\n",
        "#!pip install torch>=1.2.0\n",
        "#!pip install git+git://github.com/pytorch/audio"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zYCJHVAKkIQc",
        "outputId": "c7082f5a-4476-41df-cf5f-13a2fe261566",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install torch>=1.2.0\n",
        "!pip install torchaudio"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.7/dist-packages (0.10.0+cu111)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchaudio) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchaudio) (3.10.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9cdGy8hiOf1"
      },
      "source": [
        "#!pip list -v"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check if gpu is used\n",
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name() != ''"
      ],
      "metadata": {
        "id": "zxIalkKxXkzR",
        "outputId": "ac18272f-fa71-4260-f774-b627f6981f6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jMQcXsJ26X5s",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "666a6842-3f1c-4f36-8b0b-006d19f0757d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCfJTnnxVjTm"
      },
      "source": [
        "from typing import List\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import tqdm.notebook as tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as opt\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import torchaudio\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import IPython.display as ipd\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8FWYesjJac1_"
      },
      "source": [
        "# Если есть настроенное gpu, то можно просто cuda, либо cuda:gpu_id\n",
        "# Либо 'cpu' в случае обучения на CPU\n",
        "DEVICE = 'cuda' #'cuda:1'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uCCBrhYW3cN4"
      },
      "source": [
        "import tarfile\n",
        "\n",
        "fname = \"/content/drive/MyDrive/yandex-cup-asr-data.tar.gz\"\n",
        "\n",
        "if fname.endswith(\"tar.gz\"):\n",
        "    tar = tarfile.open(fname, \"r:gz\")\n",
        "    tar.extractall()\n",
        "    tar.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbLjQ2nwPmH8"
      },
      "source": [
        "# Путь по которому распакован архив с данными\n",
        "START_PATH = '/content/' #'./../../spotter_denoiser_data'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1K0oNwmNm69"
      },
      "source": [
        "# Список классов для предсказаний\n",
        "CLASSES = [\n",
        "    'дальше',\n",
        "    'вперед',\n",
        "    'назад',\n",
        "    'вверх',\n",
        "    'вниз',\n",
        "    'выше',\n",
        "    'ниже',\n",
        "    'домой',\n",
        "    'громче',\n",
        "    'тише',\n",
        "    'лайк',\n",
        "    'дизлайк',\n",
        "    'следующий',\n",
        "    'предыдущий',\n",
        "    'сначала',\n",
        "    'перемотай',\n",
        "    'выключи',\n",
        "    'стоп',\n",
        "    'хватит',\n",
        "    'замолчи',\n",
        "    'заткнись',\n",
        "    'останови',\n",
        "    'пауза',\n",
        "    'включи',\n",
        "    'смотреть',\n",
        "    'продолжи',\n",
        "    'играй',\n",
        "    'запусти',\n",
        "    'ноль',\n",
        "    'один',\n",
        "    'два',\n",
        "    'три',\n",
        "    'четыре',\n",
        "    'пять',\n",
        "    'шесть',\n",
        "    'семь',\n",
        "    'восемь',\n",
        "    'девять'\n",
        "]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "serC6UNfVp6p"
      },
      "source": [
        "# Данные\n",
        "\n",
        "В качестве тренировачных данных есть 2 сета:\n",
        "- чистый сет с произношением различных голосовых команд, чтобы можно было обучать на них модель голосовой активации\n",
        "- сет с шумами, чтобы можно было пытаться ими аугментировать записи для тренировки модели голосовой активации или денойзера\n",
        "\n",
        "В качестве тестовых данных используются уже зашумлённые данные.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UeVHx1HGay6d"
      },
      "source": [
        "def add_noise(clean, noise, min_amp, max_amp):\n",
        "    \"\"\"\n",
        "    Функция, которая используется для зашумления данных\n",
        "    clean и noise -- это считанные аудиозаписи с чистой речью и шумом соответственно\n",
        "    min_amp -- минимальное отношение мощности шума к мощности речи\n",
        "    max_amp -- максимальное отношение мощности шума к мощности речи\n",
        "    \"\"\"\n",
        "    # степень шума случайна от min_amp до max_amp\n",
        "    noise_amp = np.random.uniform(min_amp, max_amp)\n",
        "    # если запись с шумом короче, чем чистая, то она дублируется нужное число раз\n",
        "    noise = noise.repeat(1, clean.shape[1] // noise.shape[1] + 2)\n",
        "    # так как теперь шумная запись длиннее, то выбираем случайный момент начала шумной записи\n",
        "    start = np.random.randint(0, noise.shape[1] - clean.shape[1] + 1)\n",
        "    noise_part = noise[:, start:start+clean.shape[1]]\n",
        "    # накладываем шум\n",
        "    noise_mult = clean.abs().max() / noise_part.abs().max() * noise_amp\n",
        "    return (clean + noise_part * noise_mult) / (1 + noise_amp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeeUrvSOEjeU"
      },
      "source": [
        "class AudioDataset(Dataset):\n",
        "    \"\"\"\n",
        "    Датасет умеющий выдавать чистые + грязные данные для тренировки,\n",
        "    как модели голосовой активации, так и денойзера\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, audio_path, noise_path=None,\n",
        "        prepare_fun=torchaudio.transforms.MFCC(melkwargs={'n_mels': 80}),\n",
        "        min_amp: float = 0.0, max_amp: float = 4.0\n",
        "    ):\n",
        "        \"\"\"\n",
        "        audio_path -- Путь к чистым данны\n",
        "        noise_path -- Путь к шуму для накладывания на чистые данные, если нужен\n",
        "        min_amp -- минимальное отношение мощности шума к мощности речи\n",
        "        max_amp -- максимальное отношение мощности шума к мощности речи\n",
        "        \n",
        "        min_amp и max_amp можно варьировать, чтобы подготовить модель к различным видам зашумления.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self._prepare_fun = prepare_fun\n",
        "        self._min_amp = min_amp\n",
        "        self._max_amp = max_amp\n",
        "        self._noise = None\n",
        "        if audio_path is None:\n",
        "            assert noise_path is None\n",
        "            return\n",
        "        self._pathes, self._labels = self._path_traversall(audio_path)\n",
        "        if noise_path:\n",
        "            noise, _ = self._path_traversall(noise_path)\n",
        "            self._noise = [torchaudio.load(path)[0] for path in noise]\n",
        "\n",
        "    @classmethod\n",
        "    def _create_from_loaded(\n",
        "        cls, pathes, labels, noise, prepare_fun, min_amp, max_amp\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Создаёт новый датасет из уже подгруженного.\n",
        "        Используется при сплите датасета на train и val.\n",
        "        \"\"\"\n",
        "        obj = AudioDataset(\n",
        "            None, prepare_fun=prepare_fun, min_amp=min_amp, max_amp=max_amp\n",
        "        )\n",
        "        obj._pathes = pathes\n",
        "        obj._labels = labels\n",
        "        obj._noise = noise\n",
        "        return obj\n",
        "\n",
        "    def _path_traversall(self, path, label=None):\n",
        "        \"\"\"\n",
        "        Функция для обхода папки с данными\n",
        "        \"\"\"\n",
        "        pathes = []\n",
        "        labels = []\n",
        "        for filename in os.listdir(path):\n",
        "            if os.path.splitext(filename)[1] == '.wav':\n",
        "                pathes.append(os.path.join(path, filename))\n",
        "                labels.append(CLASSES.index(label) if label else None)\n",
        "            elif os.path.isdir(os.path.join(path, filename)):\n",
        "                new_pathes, new_labels = self._path_traversall(\n",
        "                    os.path.join(path, filename), label=filename\n",
        "                )\n",
        "                pathes.extend(new_pathes)\n",
        "                labels.extend(new_labels)\n",
        "            else:\n",
        "                raise RuntimeError(f'Unknorwn file extension for {filename}')\n",
        "        return pathes, labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self._pathes)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        audio = None\n",
        "        audio, _ = torchaudio.load(self._pathes[idx])\n",
        "        return_dict = {\n",
        "            'clean_feats': self._prepare_fun(audio)[0],\n",
        "            'clean_audio': audio,\n",
        "            'uttid': os.path.split(self._pathes[idx])[-1]\n",
        "        }\n",
        "        if self._noise is not None:\n",
        "            # Если в класс передавались шумы, то тут происходит зашумление\n",
        "            noise_idx = np.random.randint(0, len(self._noise))\n",
        "            noise_audio = add_noise(\n",
        "                audio, self._noise[noise_idx], self._min_amp, self._max_amp\n",
        "            )\n",
        "            return_dict['noise_audio'] = noise_audio\n",
        "            return_dict['noise_feats'] = self._prepare_fun(noise_audio)[0]\n",
        "        else:\n",
        "            # Если же нет, то вместо шумов выдаётся чистый звук\n",
        "            return_dict['noise_audio'] = return_dict['clean_audio']\n",
        "            return_dict['noise_feats'] = return_dict['clean_feats']\n",
        "        if self._labels[idx] is not None:\n",
        "            return_dict['label'] = self._labels[idx]\n",
        "        return return_dict\n",
        "\n",
        "    def split(self, train_part=0.9, seed=42):\n",
        "        \"\"\"\n",
        "        Функция для того чтобы разделить датасет на 2 части, например, на train и val.\n",
        "        \"\"\"\n",
        "        np.random.seed(seed)\n",
        "        train_pathes = []\n",
        "        train_labels = []\n",
        "        val_pathes = []\n",
        "        val_labels = []\n",
        "        for idx in range(len(self._pathes)):\n",
        "            if np.random.rand() < train_part:\n",
        "                train_pathes.append(self._pathes[idx])\n",
        "                train_labels.append(self._labels[idx])\n",
        "            else:\n",
        "                val_pathes.append(self._pathes[idx])\n",
        "                val_labels.append(self._labels[idx])\n",
        "        return (\n",
        "            AudioDataset._create_from_loaded(\n",
        "                train_pathes, train_labels, self._noise, self._prepare_fun,\n",
        "                self._min_amp, self._max_amp\n",
        "            ),\n",
        "            AudioDataset._create_from_loaded(\n",
        "                val_pathes, val_labels, self._noise, self._prepare_fun,\n",
        "                self._min_amp, self._max_amp\n",
        "            )\n",
        "        )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J3IZyGcR0Ka"
      },
      "source": [
        "# Подгрузим тренировочный сет в двух вариантах: чистый и с зашумлением\n",
        "fullcleanset = AudioDataset(os.path.join(START_PATH, 'speech_commands_train'))\n",
        "traincleanset, valcleanset = fullcleanset.split()\n",
        "fullnoiseset = AudioDataset(\n",
        "    os.path.join(START_PATH, 'speech_commands_train'), noise_path=os.path.join(START_PATH, 'noises')\n",
        ")\n",
        "trainnoiseset, valnoiseset = fullnoiseset.split()\n",
        "testset = AudioDataset(os.path.join(START_PATH, 'speech_commands_test'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABkq7PYEE_mj"
      },
      "source": [
        "## Как выглядят полученные данные\n",
        "\n",
        "Вот например чистые аудиофайлы:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rOx-vSaSb2ek"
      },
      "source": [
        "random_idxes = np.random.randint(0, len(fullnoiseset), size=10)\n",
        "for idx in random_idxes:\n",
        "    ipd.display(ipd.Audio(fullnoiseset[idx]['clean_audio'], rate=16000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vn9CuN0aebQU"
      },
      "source": [
        "А тут уже зашумлённые."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHijPdxPc0VU"
      },
      "source": [
        "for idx in random_idxes:\n",
        "    ipd.display(ipd.Audio(fullnoiseset[idx]['noise_audio'], rate=16000))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEy-U-62HVux"
      },
      "source": [
        "MAX_LEN = 0\n",
        "for i in tqdm.trange(len(fullcleanset)):\n",
        "    MAX_LEN = max(fullcleanset[i]['clean_feats'].shape[1], MAX_LEN)\n",
        "print('max len of audio equal to', MAX_LEN, 'frames')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4JHkUpUuzxj_"
      },
      "source": [
        "Несколько функций для того, чтобы покрыть нужную информацию в батче для разных тренировок."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxovwz7MMjQy"
      },
      "source": [
        "def join_in_batch(batch, key):\n",
        "    \"\"\"\n",
        "    Фунция для того, чтобы создать батч из аудиозаписей разной длины.\n",
        "    \"\"\"\n",
        "    X = torch.zeros((len(batch), batch[0][key].shape[0], MAX_LEN))\n",
        "    for i in range(len(batch)):\n",
        "        diff = MAX_LEN - batch[i][key].shape[1]\n",
        "        start = diff // 2\n",
        "        end = MAX_LEN - diff + start\n",
        "        if start != 0:\n",
        "            X[i, :, :start] = batch[i][key][:, 0].reshape(-1, 1)\n",
        "        X[i, :, start:end] = batch[i][key]\n",
        "        if end != MAX_LEN:\n",
        "            X[i, :, end:] = batch[i][key][:, -1].reshape(-1, 1)\n",
        "    return X\n",
        "\n",
        "\n",
        "# Дальше функции для того, чтобы склеить записи в один батч на все случаи жизни\n",
        "\n",
        "def spotter_collate_fn(batch):\n",
        "    X = join_in_batch(batch, 'noise_feats')\n",
        "    Y = torch.tensor([item['label'] for item in batch])\n",
        "    return X, Y\n",
        "\n",
        "def denoiser_collate_fn(batch):\n",
        "    X = join_in_batch(batch, 'noise_feats')\n",
        "    Y = join_in_batch(batch, 'clean_feats')\n",
        "    return X, Y\n",
        "\n",
        "def test_collate_fn(batch):\n",
        "    X = join_in_batch(batch, 'noise_feats')\n",
        "    Y = [item['uttid'] for item in batch]\n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExN_dcuoUHVg"
      },
      "source": [
        "# Модель голосовой активации\n",
        "\n",
        "Модель голосовой активации на входе принимает матрицу из признаков размера (время $\\times$ количество признаков). Поверх этого можно запустить достаточно простую свёрточную модель с 1d свёртками, ибо считаем, что нам интересны локальные зависимости в пространстве времени, а пространство признаков становится чем-то вроде пространства каналов в случае картинок."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gaRNDOgBXmvj"
      },
      "source": [
        "def spotter_eval(model, val_data, batch_size=256):\n",
        "    \"\"\"\n",
        "    Фунция подсчёта качества на валидационных модели на валидационных данных.\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    val_loader = DataLoader(\n",
        "        val_data, batch_size=batch_size, shuffle=True, num_workers=8,\n",
        "        collate_fn=spotter_collate_fn\n",
        "    )\n",
        "    val_loss, val_acc, val_batches = 0, 0, 0\n",
        "    for X, Y in tqdm.tqdm(val_loader):\n",
        "        preds = model.forward(X.to(DEVICE))\n",
        "        loss = F.cross_entropy(preds, Y.to(DEVICE))\n",
        "        val_loss += loss.detach().cpu().data.numpy()\n",
        "        val_acc += np.mean((torch.argmax(preds, dim=-1).cpu() == Y).data.numpy())\n",
        "        val_batches += 1\n",
        "    return val_loss / val_batches, val_acc / val_batches\n",
        "    \n",
        "\n",
        "def spotter_train(model, train_data, val_data, n_epochs=10, batch_size=256):\n",
        "    \"\"\"\n",
        "    Простая тренировка поданной на вход модели.\n",
        "    В качестве оптимизатора используется Adam.\n",
        "    Тренировка происходит заданное число эпох с константным lr.\n",
        "    \"\"\"\n",
        "    optimizer = opt.Adam(model.parameters())\n",
        "    train_loader = DataLoader(\n",
        "        train_data, batch_size=batch_size, shuffle=True, num_workers=8,\n",
        "        collate_fn=spotter_collate_fn\n",
        "    )\n",
        "    train_losses, train_accs = [], []\n",
        "    val_losses,val_accs = [], []\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_acc, train_batches = 0, 0, 0\n",
        "        model.train()\n",
        "        for X, Y in tqdm.tqdm(train_loader):\n",
        "            preds = model.forward(X.to(DEVICE))\n",
        "            loss = F.cross_entropy(preds, Y.to(DEVICE))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.cpu().data.numpy()\n",
        "            train_acc += np.mean((torch.argmax(preds, dim=-1).cpu() == Y).data.numpy())\n",
        "            train_batches += 1\n",
        "        val_loss, val_acc = spotter_eval(model, val_data, batch_size=batch_size)\n",
        "        train_losses.append(train_loss / train_batches)\n",
        "        train_accs.append(train_acc / train_batches)\n",
        "        val_losses.append(val_loss)\n",
        "        val_accs.append(val_acc)\n",
        "        clear_output(wait=True)\n",
        "        fig, axis = plt.subplots(1, 2, figsize=(15, 7))\n",
        "        axis[0].plot(train_losses, label='train')\n",
        "        axis[0].plot(val_losses, label='val')\n",
        "        axis[1].plot(train_accs, label='train')\n",
        "        axis[1].plot(val_accs, label='val')\n",
        "        (ax.set_xlabel('epoch') for ax in axis)\n",
        "        axis[0].set_ylabel('cross entropy loss')\n",
        "        axis[1].set_ylabel('accuracy')\n",
        "        (ax.legend() for ax in axis)\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swIssDBuSEta"
      },
      "source": [
        "class Conv1dModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Простой кирпичик для свёрточной модели.\\\n",
        "    n_in -- число фильтров на входе\n",
        "    n_out -- число фильтров на выходе\n",
        "    kernel -- размер ядра\n",
        "    pooling -- размер ядра пулинга\n",
        "    batchnorm -- флаг отвечающий за использование батч нормализации\n",
        "    relu -- флаг отвечающий за использование нелинейности\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out, kernel, pooling, batchnorm=False, relu=True):\n",
        "        super().__init__()\n",
        "        assert kernel % 2 == 1\n",
        "        pad = kernel // 2\n",
        "        modules = [nn.Conv1d(n_in, n_out, kernel, padding=pad)]\n",
        "        if batchnorm:\n",
        "            modules.append(nn.BatchNorm1d(n_out))\n",
        "        if pooling > 1:\n",
        "            modules.append(nn.MaxPool1d(pooling))\n",
        "        if relu:\n",
        "            modules.append(nn.ReLU())\n",
        "        self._net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self._net.forward(X)\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, X):\n",
        "        return X.reshape(X.shape[0], -1)\n",
        "\n",
        "class FlattenModule(nn.Module):\n",
        "    \"\"\"\n",
        "    Простой кирпичик полносвязной части свёрточной модели.\n",
        "    n_in -- число нейронов на входе\n",
        "    n_out -- число нейроново на выходе\n",
        "    batchnorm -- флаг отвечающий за использование батч нормализации\n",
        "    relu -- флаг отвечающий за использование нелинейности\n",
        "    \"\"\"\n",
        "    def __init__(self, n_in, n_out, batchnorm=False, relu=True):\n",
        "        super().__init__()\n",
        "        modules = [nn.Linear(n_in, n_out)]\n",
        "        if batchnorm:\n",
        "            modules.append(nn.BatchNorm1d(n_out))\n",
        "        if relu:\n",
        "            modules.append(nn.ReLU())\n",
        "        self._net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self._net.forward(X)\n",
        "\n",
        "class Conv1dModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Пример простой сети для работы со звуком.\n",
        "    Изначально несколько слоёв используют 1d свёртки, затем просходит конкатенация матрицы\n",
        "    признаков в вектор и применяются несколько полносвязных слоёв.\n",
        "    В целом тут всё очень сильно похоже на картинки.\n",
        "    \n",
        "    shapes -- число филтров в свёрточных слоях. На нулевом индексе число фильтров на входе.\n",
        "        Далее число фильтров после каждого свёрточного слоя\n",
        "    flatten_shapes -- число нейронов после применения линейных слоёв.\n",
        "    kernels -- размеры ядер свёрточных слоёв\n",
        "    poolings -- параметры пулинга после свёрточных слоёв\n",
        "    batchnorm -- флаг отвечающий за использовать или нет нормализацию\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self, shapes: List[int], flatten_shapes: List[int], kernels: List[int],\n",
        "        poolings: List[int], batchnorm=False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        assert len(kernels) + 1 == len(shapes)\n",
        "        assert len(poolings) == len(kernels)\n",
        "        modules = []\n",
        "        start_flatten_shape = MAX_LEN\n",
        "        for i in range(len(kernels)):\n",
        "            modules.append(Conv1dModule(\n",
        "                shapes[i], shapes[i + 1], kernels[i], poolings[i],\n",
        "                batchnorm=batchnorm\n",
        "            ))\n",
        "            start_flatten_shape //= poolings[i]\n",
        "        modules.append(Flatten())\n",
        "        flatten_shapes = [start_flatten_shape * shapes[-1]] + flatten_shapes\n",
        "        for i in range(len(flatten_shapes) - 1):\n",
        "            modules.append(FlattenModule(\n",
        "                flatten_shapes[i], flatten_shapes[i + 1], batchnorm=batchnorm, relu=i+2==len(flatten_shapes)\n",
        "            ))\n",
        "        self._net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self._net.forward(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ff6fMmu6zxkD"
      },
      "source": [
        "Сразу попробуем натренировать модель на чистых данных и применить на грязных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i134VCckddHu"
      },
      "source": [
        "conv1d_clean_model = Conv1dModel(\n",
        "    [40] + [256] * 7, [1024, 256, len(CLASSES)],\n",
        "    [5] * 7, [1, 2] * 3 + [1], batchnorm=True\n",
        ").to(DEVICE)\n",
        "spotter_train(conv1d_clean_model, traincleanset, valcleanset, n_epochs=10, batch_size=256)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1h4yL3nzxkG"
      },
      "source": [
        "spotter_eval(conv1d_clean_model, valcleanset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xAo1sdwCzxkH"
      },
      "source": [
        "spotter_eval(conv1d_clean_model, valnoiseset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lIg8NK0dzxkI"
      },
      "source": [
        "Как видно, модель натренированная на чистых данных, довольно плохо работает на грязных данных. Так что можно попробовать как минимум 2 выхода: натренировать модель на грязных данных и натренировать денойзер убирающий шум из грязных данных."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF0VDQP6PWC_"
      },
      "source": [
        "## Тренировка на грязных данных"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "uqOGloDgzxkI"
      },
      "source": [
        "conv1d_noise_model = Conv1dModel(\n",
        "    [40] + [256] * 7, [1024, 256, len(CLASSES)],\n",
        "    [5] * 7, [2] * 7, batchnorm=True\n",
        ").to(DEVICE)\n",
        "spotter_train(conv1d_noise_model, trainnoiseset, valnoiseset, n_epochs=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6xsiIfdshny"
      },
      "source": [
        "spotter_train(conv1d_noise_model, trainnoiseset, valnoiseset, n_epochs=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Gxh76DIzxkJ"
      },
      "source": [
        "spotter_eval(conv1d_noise_model, valnoiseset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_stS_hnip1M"
      },
      "source": [
        "torch.save(conv1d_noise_model.state_dict(), 'noise_model3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8atgclMjBMt"
      },
      "source": [
        "#model = TheModelClass(*args, **kwargs)\n",
        "#model.load_state_dict(torch.load(PATH))\n",
        "#model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s8Dyb1im139n"
      },
      "source": [
        "# Попробуем добавить денойзер"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5iMPLULqKmWx"
      },
      "source": [
        "def denoiser_train(model, train_data, val_data, n_epochs=10, batch_size=256):\n",
        "    optimizer = opt.Adam(model.parameters())\n",
        "    train_loader = DataLoader(\n",
        "        train_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "        collate_fn=denoiser_collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_data, batch_size=batch_size, shuffle=True, num_workers=4,\n",
        "        collate_fn=denoiser_collate_fn\n",
        "    )\n",
        "    train_losses, val_losses = [], []\n",
        "    for epoch in range(n_epochs):\n",
        "        train_loss, train_batches = 0, 0\n",
        "        model.train()\n",
        "        for X, Y in tqdm.tqdm(train_loader):\n",
        "            preds = model.forward(X.to(DEVICE))\n",
        "            loss = F.mse_loss(preds, Y.to(DEVICE))\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss += loss.cpu().data.numpy()\n",
        "            train_batches += 1\n",
        "        val_loss, val_batches = 0, 0\n",
        "        model.eval()\n",
        "        for X, Y in tqdm.tqdm(val_loader):\n",
        "            preds = model.forward(X.to(DEVICE))\n",
        "            loss = F.mse_loss(preds, Y.to(DEVICE))\n",
        "            val_loss += loss.detach().cpu().data.numpy()\n",
        "            val_batches += 1\n",
        "        train_losses.append(train_loss / train_batches)\n",
        "        val_losses.append(val_loss / val_batches)\n",
        "        clear_output(wait=True)\n",
        "        plt.plot(train_losses, label='train')\n",
        "        plt.plot(val_losses, label='val')\n",
        "        plt.xlabel('epoch')\n",
        "        plt.ylabel('cross entropy loss')\n",
        "        plt.legend()\n",
        "        plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZswhKOLyznwg"
      },
      "source": [
        "class SimpleDenoiserModel(nn.Module):\n",
        "    \"\"\"\n",
        "    Очень простой денойзер состоящий из маленькой свёрточной модели.\n",
        "    shapes -- число фильтров в свёрточных слоях. На нулевом индексе число фильтров на входе\n",
        "    kernels -- размерность ядер в свёрточных слоях\n",
        "    batchnorm -- флаг отвечающий за использование нормализации\n",
        "    \"\"\"\n",
        "    def __init__(self, shapes: List[int], kernels: List[int], batchnorm=False):\n",
        "        super().__init__()\n",
        "        assert len(shapes) == len(kernels) + 1\n",
        "        modules = []\n",
        "        for i in range(len(kernels)):\n",
        "            modules.append(Conv1dModule(\n",
        "                shapes[i], shapes[i + 1], kernels[i], 1, batchnorm=batchnorm, relu=i + 1!=len(kernels)\n",
        "            ))\n",
        "        self._net = nn.Sequential(*modules)\n",
        "\n",
        "    def forward(self, X):\n",
        "        return self._net.forward(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htUW9S6bzxkK"
      },
      "source": [
        "def eval_spotter_with_denoiser(denoiser, spotter, val_data, batch_size=256):\n",
        "    \"\"\"\n",
        "    Функция которую можно использовать для того, чтобы поэвалить споттер на звуке\n",
        "    после применения денойзера.\n",
        "    \"\"\"\n",
        "    denoiser.eval()\n",
        "    spotter.eval()\n",
        "    val_loader = DataLoader(\n",
        "        val_data, batch_size=batch_size, shuffle=True, num_workers=8,\n",
        "        collate_fn=spotter_collate_fn\n",
        "    )\n",
        "    val_loss, val_acc, val_batches = 0, 0, 0\n",
        "    for X, Y in tqdm.tqdm(val_loader):\n",
        "        clean = denoiser.forward(X.to(DEVICE))\n",
        "        preds = spotter.forward(clean)\n",
        "        loss = F.cross_entropy(preds, Y.to(DEVICE))\n",
        "        val_loss += loss.detach().cpu().data.numpy()\n",
        "        val_acc += np.mean((torch.argmax(preds, dim=-1).cpu() == Y).data.numpy())\n",
        "        val_batches += 1\n",
        "    return val_loss / val_batches, val_acc / val_batches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaPS0HNvzxkK"
      },
      "source": [
        "model = SimpleDenoiserModel([40, 64, 128, 64, 40], [3] * 4).to(DEVICE)\n",
        "denoiser_train(model, trainnoiseset, valnoiseset, n_epochs=50)\n",
        "eval_spotter_with_denoiser(model, conv1d_clean_model, valnoiseset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5872riAzxkK"
      },
      "source": [
        "Как видно качество тут стало лучше, чем после применения на грязных данных модели, которая тренировалась на чистых. Но вот сходу до качества модели, натренированной на грязных данных этот подход не дотягивает."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyUjaGxkzxkL"
      },
      "source": [
        "# Отправка решения\n",
        "\n",
        "Сгенерим ответы с помощью самой лучшей модели, которая тут получилась"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O01e1JPrzxkL"
      },
      "source": [
        "def final_eval(\n",
        "    spotter, test_data, denoiser=None,\n",
        "    save_path=os.path.join(START_PATH, 'results.tsv'), batch_size=256\n",
        "):\n",
        "    spotter.eval()\n",
        "    if denoiser:\n",
        "        denoiser.eval()\n",
        "    test_loader = DataLoader(\n",
        "        test_data, batch_size=batch_size, shuffle=True, num_workers=2,\n",
        "        collate_fn=test_collate_fn\n",
        "    )\n",
        "    val_loss, val_acc, val_batches = 0, 0, 0\n",
        "    results = []\n",
        "    for X, Y in tqdm.tqdm(test_loader):\n",
        "        X = X.to(DEVICE)\n",
        "        if denoiser:\n",
        "            X = denoiser.forward(X)\n",
        "        preds = spotter.forward(X)\n",
        "        classes = torch.argmax(preds, dim=-1).cpu().data.numpy()\n",
        "        labels = [CLASSES[idx] for idx in classes]\n",
        "        results.extend([uttid, label] for uttid, label in zip(Y, labels))\n",
        "    with open(save_path, 'w') as fout:\n",
        "        for uttid, label in results:\n",
        "            fout.write(f'{uttid}\\t{label}\\n')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vM8cnboCzxkL"
      },
      "source": [
        "final_eval(conv1d_noise_model, testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8iwiPFwdzxkL"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}